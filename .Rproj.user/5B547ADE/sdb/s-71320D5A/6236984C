{
    "contents" : "ModelData <- na.omit(ModelData)\n\n\nModelDataScaled <- ModelDataFactored\nTestDataScaled <- TestDataFactored\n\nModelDataScaled <- sapply(ModelDataScaled[,-33], as.numeric)\nModelDataScaled <- as.data.frame(scale(ModelDataScaled[,-33]))\nModelDataScaled$CKD <- ModelDataFactored$CKD\n\nTestDataScaled <- sapply(TestDataScaled, as.numeric)\nTestDataScaled <- as.data.frame(scale(TestDataScaled))\n\nset.seed(0305)\nindex <- createDataPartition(ModelDataFactored$CKD, p = 0.75, list = FALSE)\ntraining <- ModelDataScaled[index,]\ntesting <- ModelDataScaled[-index,]\n\n\n#Now let's try finding the best k\nctrl <- trainControl(method = \"repeatedcv\", number = 3, repeats = 3)\nknnFit <- train(CKD~., data = training, method = \"knn\", trControl = ctrl, tuneLength = 20)\nknnFit\nprint(\"Final value of k = 19\")\n\n#training$CKD <- factor(training$CKD, labels = c(\"Yes\", \"No\"), levels=c(1,0))\n#testing$CKD <- factor(testing$CKD, labels = c(\"Yes\", \"No\"), levels=c(1,0))\n\nKNN_Final <- knn(training[,-33], testing[,-33], cl = training$CKD, k = 17)\ntable(testing$CKD, KNN_Final)\nconfusionMatrix(KNN_Final, testing$CKD)\n\nprint(\"K-Nearest Neighbors results:\")\nprint(\"Accuracy: 93%\")\nprint(\"Sensitivity: 100%\")\nprint(\"Specificity: 0%\")\n\nKNN_Pred <- knn(ModelDataScaled[, -33], TestDataScaled, cl = ModelDataScaled$CKD, k = 17)\nTestDataKNN <- TestDataFactored\nTestDataKNN$CKD <- KNN_Pred\nwrite.csv(TestDataKNN, file = \"KNN.csv\", row.names = F, quote = F)\n\nModelDataScaled <- NULL\nTestDataScaled <- NULL\ntraining <- NULL\ntesting <- NULL\n",
    "created" : 1449695074278.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1272997544",
    "id" : "6236984C",
    "lastKnownWriteTime" : 1449668513,
    "path" : "C:/Users/babaj/Desktop/Code/CKD-Tessilk/KNN.R",
    "project_path" : "KNN.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "type" : "r_source"
}